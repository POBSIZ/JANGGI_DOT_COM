#!/usr/bin/env python3
"""Smart NNUE Training Script - ìë™ í™˜ê²½ ê°ì§€ ë° ìµœì í™” í•™ìŠµ.

ì»´í“¨í„° í™˜ê²½ì„ ìë™ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ ìµœì ì˜ í•™ìŠµ ë°©ì‹ì„ ì„ íƒí•©ë‹ˆë‹¤.
í•™ìŠµ ì‹œê°„ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ì‹œê°„ì— ë§ëŠ” ì„¤ì •ìœ¼ë¡œ ìë™ í•™ìŠµí•©ë‹ˆë‹¤.

Usage:
    # ëŒ€í™”í˜• ëª¨ë“œ (ê¶Œì¥)
    python smart_train.py
    
    # ì§ì ‘ ì‹œê°„ ì„ íƒ
    python smart_train.py --time quick       # ~5ë¶„
    python smart_train.py --time standard    # ~15ë¶„  
    python smart_train.py --time deep        # ~30ë¶„
    python smart_train.py --time intensive   # ~1ì‹œê°„
    python smart_train.py --time full        # ~2ì‹œê°„+
    
    # ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµ
    python smart_train.py --load models/nnue_model.json --time standard
"""

import argparse
import os
import sys
import platform
import time
import glob
from typing import Dict, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class TrainingTime(Enum):
    """í•™ìŠµ ì‹œê°„ ì˜µì…˜"""
    QUICK = "quick"           # ~5ë¶„
    STANDARD = "standard"     # ~15ë¶„
    DEEP = "deep"             # ~30ë¶„
    INTENSIVE = "intensive"   # ~1ì‹œê°„
    FULL = "full"             # ~2ì‹œê°„+


@dataclass
class SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´"""
    os_name: str
    cpu_name: str
    cpu_cores: int
    cpu_threads: int
    ram_gb: float
    gpu_available: bool
    gpu_type: str  # 'cuda', 'mps', 'none'
    gpu_name: str
    gpu_memory_gb: float
    has_gibo_files: bool
    gibo_file_count: int
    gpu_error_message: Optional[str] = None


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    method: str  # 'gpu', 'cpu', 'gibo'
    positions: int
    epochs: int
    batch_size: int
    learning_rate: float
    search_depth: int
    iterations: int  # for iterative training
    use_parallel: bool
    num_workers: int
    use_gibo: bool
    estimated_time_min: int


def get_system_info(gibo_dir: str = "gibo") -> SystemInfo:
    """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
    import multiprocessing
    import subprocess
    
    # OS ì •ë³´
    os_name = f"{platform.system()} {platform.release()}"
    
    # CPU ì •ë³´
    cpu_name = platform.processor() or "Unknown CPU"
    cpu_cores = multiprocessing.cpu_count()
    
    # ë¬¼ë¦¬ì  ì½”ì–´ vs ë…¼ë¦¬ì  ìŠ¤ë ˆë“œ
    try:
        import psutil
        cpu_threads = psutil.cpu_count(logical=True)
        cpu_cores_physical = psutil.cpu_count(logical=False) or cpu_cores
    except ImportError:
        cpu_threads = cpu_cores
        cpu_cores_physical = cpu_cores
    
    # RAM ì •ë³´
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / (1024 ** 3)
    except ImportError:
        # macOSì—ì„œ sysctl ì‚¬ìš©
        try:
            if platform.system() == "Darwin":
                result = subprocess.run(
                    ["sysctl", "-n", "hw.memsize"],
                    capture_output=True, text=True
                )
                ram_gb = int(result.stdout.strip()) / (1024 ** 3)
            elif platform.system() == "Linux":
                with open("/proc/meminfo", "r") as f:
                    for line in f:
                        if line.startswith("MemTotal:"):
                            ram_kb = int(line.split()[1])
                            ram_gb = ram_kb / (1024 ** 2)
                            break
            else:
                ram_gb = 8.0  # ê¸°ë³¸ê°’
        except:
            ram_gb = 8.0  # ê¸°ë³¸ê°’
    
    # GPU ì •ë³´
    gpu_available = False
    gpu_type = "none"
    gpu_name = "None"
    gpu_memory_gb = 0.0
    gpu_error_message = None
    
    try:
        import torch
        # PyTorchê°€ ì„±ê³µì ìœ¼ë¡œ importë˜ì—ˆëŠ”ì§€ í™•ì¸
        if torch.cuda.is_available():
            gpu_available = True
            gpu_type = "cuda"
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            gpu_available = True
            gpu_type = "mps"
            gpu_name = "Apple Silicon (MPS)"
            # MPS doesn't report memory directly, estimate based on system
            try:
                import psutil
                # Apple Silicon shares memory with system
                gpu_memory_gb = psutil.virtual_memory().total / (1024 ** 3) * 0.5
            except:
                gpu_memory_gb = 8.0  # Default estimate
        else:
            # PyTorchëŠ” ì„¤ì¹˜ë˜ì–´ ìˆì§€ë§Œ CUDA/MPSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
            torch_version = torch.__version__
            if "+cpu" in torch_version:
                gpu_error_message = f"PyTorch CPU-only ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ({torch_version}). GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ CUDA ì§€ì› ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”:\n  uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
            else:
                gpu_error_message = "PyTorchëŠ” ì„¤ì¹˜ë˜ì–´ ìˆì§€ë§Œ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CUDA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
    except (ImportError, AttributeError, RuntimeError) as e:
        # PyTorch import ì‹¤íŒ¨ ë˜ëŠ” ë‚´ë¶€ ì˜¤ë¥˜ (ì˜ˆ: AcceleratorError ë“±)
        error_type = type(e).__name__
        if isinstance(e, ImportError):
            gpu_error_message = "PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ PyTorchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:\n  uv sync --extra gpu\në˜ëŠ” CUDA ì§€ì› ë²„ì „:\n  uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
        else:
            # PyTorch ì„¤ì¹˜ê°€ ì†ìƒë˜ì—ˆê±°ë‚˜ í˜¸í™˜ì„± ë¬¸ì œ
            gpu_error_message = f"PyTorch ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤ ({error_type}: {str(e)}).\n  PyTorchë¥¼ ì¬ì„¤ì¹˜í•˜ì„¸ìš”:\n  uv pip install --force-reinstall torch\n  ë˜ëŠ” CUDA ì§€ì› ë²„ì „:\n  uv pip install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
    
    # ê¸°ë³´ íŒŒì¼ í™•ì¸
    gibo_files = glob.glob(os.path.join(gibo_dir, "*.gib")) + glob.glob(os.path.join(gibo_dir, "*.GIB"))
    has_gibo_files = len(gibo_files) > 0
    gibo_file_count = len(gibo_files)
    
    return SystemInfo(
        os_name=os_name,
        cpu_name=cpu_name,
        cpu_cores=cpu_cores_physical,
        cpu_threads=cpu_threads,
        ram_gb=ram_gb,
        gpu_available=gpu_available,
        gpu_type=gpu_type,
        gpu_name=gpu_name,
        gpu_memory_gb=gpu_memory_gb,
        has_gibo_files=has_gibo_files,
        gibo_file_count=gibo_file_count,
        gpu_error_message=gpu_error_message
    )


def print_system_info(info: SystemInfo):
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„")
    print("=" * 60)
    
    print(f"\nğŸ“Œ ìš´ì˜ì²´ì œ: {info.os_name}")
    print(f"ğŸ“Œ CPU: {info.cpu_name}")
    print(f"   - ì½”ì–´: {info.cpu_cores}ê°œ / ìŠ¤ë ˆë“œ: {info.cpu_threads}ê°œ")
    print(f"ğŸ“Œ RAM: {info.ram_gb:.1f} GB")
    
    if info.gpu_available:
        print(f"ğŸ“Œ GPU: {info.gpu_name} ({'CUDA' if info.gpu_type == 'cuda' else 'MPS'})")
        print(f"   - VRAM: {info.gpu_memory_gb:.1f} GB")
        print("   âœ… GPU ê°€ì† ì‚¬ìš© ê°€ëŠ¥")
    else:
        print("ğŸ“Œ GPU: ì‚¬ìš© ë¶ˆê°€ (CPU í•™ìŠµ ëª¨ë“œ)")
        if info.gpu_error_message:
            print(f"   âš ï¸  {info.gpu_error_message}")
    
    if info.has_gibo_files:
        print(f"ğŸ“Œ ê¸°ë³´ íŒŒì¼: {info.gibo_file_count}ê°œ ë°œê²¬")
        print("   âœ… ê¸°ë³´ ê¸°ë°˜ í•™ìŠµ ê°€ëŠ¥")
    else:
        print("ğŸ“Œ ê¸°ë³´ íŒŒì¼: ì—†ìŒ (self-play í•™ìŠµ)")


def get_training_config(info: SystemInfo, training_time: TrainingTime, use_gibo: bool = True) -> TrainingConfig:
    """ì‹œìŠ¤í…œ í™˜ê²½ê³¼ í•™ìŠµ ì‹œê°„ì— ë”°ë¥¸ ìµœì  ì„¤ì • ê³„ì‚°"""
    
    # ê¸°ë³¸ ì„¤ì •ê°’ (ì‹œê°„ë³„)
    time_configs = {
        TrainingTime.QUICK: {
            "positions": 2000,
            "epochs": 15,
            "batch_size": 128,
            "lr": 0.001,
            "depth": 2,
            "iterations": 1,
            "estimated_min": 5
        },
        TrainingTime.STANDARD: {
            "positions": 5000,
            "epochs": 30,
            "batch_size": 256,
            "lr": 0.0008,
            "depth": 2,
            "iterations": 2,
            "estimated_min": 15
        },
        TrainingTime.DEEP: {
            "positions": 10000,
            "epochs": 50,
            "batch_size": 256,
            "lr": 0.0005,
            "depth": 3,
            "iterations": 3,
            "estimated_min": 30
        },
        TrainingTime.INTENSIVE: {
            "positions": 20000,
            "epochs": 80,
            "batch_size": 512,
            "lr": 0.0003,
            "depth": 3,
            "iterations": 5,
            "estimated_min": 60
        },
        TrainingTime.FULL: {
            "positions": 50000,
            "epochs": 100,
            "batch_size": 512,
            "lr": 0.0002,
            "depth": 4,
            "iterations": 8,
            "estimated_min": 120
        }
    }
    
    config = time_configs[training_time]
    
    # GPU ê°€ìš© ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë° í¬ì§€ì…˜ ìˆ˜ ì¦ê°€
    if info.gpu_available:
        method = "gpu"
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
        if info.gpu_memory_gb >= 8:
            config["batch_size"] = min(config["batch_size"] * 2, 1024)
            config["positions"] = int(config["positions"] * 1.5)
        elif info.gpu_memory_gb >= 4:
            config["batch_size"] = min(config["batch_size"] * 1.5, 512)
        
        # GPU í•™ìŠµì€ ë” ë¹ ë¥´ë¯€ë¡œ ì‹œê°„ ì˜ˆìƒ ì¡°ì •
        config["estimated_min"] = int(config["estimated_min"] * 0.5)
    else:
        method = "cpu"
        # CPU ì½”ì–´ì— ë”°ë¼ ë³‘ë ¬í™” ì„¤ì •
        if info.cpu_cores >= 4:
            config["batch_size"] = min(config["batch_size"], 128)
        else:
            config["batch_size"] = min(config["batch_size"], 64)
            config["positions"] = int(config["positions"] * 0.7)
    
    # ê¸°ë³´ íŒŒì¼ ì‚¬ìš© ì—¬ë¶€
    should_use_gibo = use_gibo and info.has_gibo_files and info.gibo_file_count >= 5
    if should_use_gibo:
        method = "gibo" if not info.gpu_available else "gpu_gibo"
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    use_parallel = info.cpu_cores >= 4
    num_workers = max(1, min(info.cpu_cores - 1, 8))
    
    # RAMì´ ì ìœ¼ë©´ ì„¤ì • ì¡°ì •
    if info.ram_gb < 8:
        config["positions"] = int(config["positions"] * 0.5)
        config["batch_size"] = min(config["batch_size"], 128)
        num_workers = min(num_workers, 2)
    
    return TrainingConfig(
        method=method,
        positions=int(config["positions"]),
        epochs=int(config["epochs"]),
        batch_size=int(config["batch_size"]),
        learning_rate=config["lr"],
        search_depth=config["depth"],
        iterations=config["iterations"],
        use_parallel=use_parallel,
        num_workers=num_workers,
        use_gibo=should_use_gibo,
        estimated_time_min=config["estimated_min"]
    )


def print_training_config(config: TrainingConfig):
    """í•™ìŠµ ì„¤ì • ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("âš™ï¸  í•™ìŠµ ì„¤ì •")
    print("=" * 60)
    
    method_names = {
        "gpu": "GPU ê°€ì† í•™ìŠµ",
        "cpu": "CPU í•™ìŠµ",
        "gibo": "ê¸°ë³´ ê¸°ë°˜ í•™ìŠµ (CPU)",
        "gpu_gibo": "ê¸°ë³´ ê¸°ë°˜ í•™ìŠµ (GPU)"
    }
    
    print(f"\nğŸ“‹ í•™ìŠµ ë°©ì‹: {method_names.get(config.method, config.method)}")
    print(f"ğŸ“‹ í•™ìŠµ í¬ì§€ì…˜ ìˆ˜: {config.positions:,}ê°œ")
    print(f"ğŸ“‹ ì—í¬í¬ ìˆ˜: {config.epochs}íšŒ")
    print(f"ğŸ“‹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config.batch_size}")
    print(f"ğŸ“‹ í•™ìŠµë¥ : {config.learning_rate}")
    print(f"ğŸ“‹ íƒìƒ‰ ê¹Šì´: {config.search_depth}")
    
    if config.iterations > 1:
        print(f"ğŸ“‹ ë°˜ë³µ í•™ìŠµ: {config.iterations}íšŒ")
    
    if config.use_parallel:
        print(f"ğŸ“‹ ë³‘ë ¬ ì²˜ë¦¬: {config.num_workers}ê°œ ì›Œì»¤")
    
    if config.use_gibo:
        print("ğŸ“‹ ê¸°ë³´ ë°ì´í„° í™œìš©: âœ…")
    
    print(f"\nâ±ï¸  ì˜ˆìƒ í•™ìŠµ ì‹œê°„: ì•½ {config.estimated_time_min}ë¶„")


def interactive_menu(info: SystemInfo) -> Tuple[TrainingTime, bool, Optional[str]]:
    """ëŒ€í™”í˜• ë©”ë‰´"""
    print("\n" + "=" * 60)
    print("ğŸ¯ í•™ìŠµ ì‹œê°„ ì„ íƒ")
    print("=" * 60)
    
    options = [
        (TrainingTime.QUICK, "âš¡ ë¹ ë¥¸ í•™ìŠµ", "~5ë¶„", "ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©, ê¸°ë³¸ì ì¸ í•™ìŠµ"),
        (TrainingTime.STANDARD, "ğŸ“˜ í‘œì¤€ í•™ìŠµ", "~15ë¶„", "ì¼ë°˜ì ì¸ ì‚¬ìš©ì— ì í•©"),
        (TrainingTime.DEEP, "ğŸ“— ê¹Šì€ í•™ìŠµ", "~30ë¶„", "ë” ë‚˜ì€ ì„±ëŠ¥, ê¶Œì¥"),
        (TrainingTime.INTENSIVE, "ğŸ“• ì§‘ì¤‘ í•™ìŠµ", "~1ì‹œê°„", "ë†’ì€ ì„±ëŠ¥ ëª©í‘œ"),
        (TrainingTime.FULL, "ğŸ† ì™„ì „ í•™ìŠµ", "~2ì‹œê°„+", "ìµœê³  ì„±ëŠ¥, ì‹œê°„ ì—¬ìœ  ìˆì„ ë•Œ"),
    ]
    
    print("\ní•™ìŠµ ì‹œê°„ì„ ì„ íƒí•˜ì„¸ìš”:\n")
    for i, (_, name, time_est, desc) in enumerate(options, 1):
        print(f"  {i}. {name} ({time_est})")
        print(f"     â””â”€ {desc}")
    
    print("\n  0. ì¢…ë£Œ")
    
    while True:
        try:
            choice = input("\nì„ íƒ (1-5, 0=ì¢…ë£Œ): ").strip()
            if choice == "0":
                return None, False, None
            
            idx = int(choice) - 1
            if 0 <= idx < len(options):
                selected_time = options[idx][0]
                break
            print("âŒ 1-5 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # ê¸°ë³´ ì‚¬ìš© ì—¬ë¶€
    use_gibo = False
    if info.has_gibo_files:
        print(f"\nê¸°ë³´ íŒŒì¼ {info.gibo_file_count}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        gibo_choice = input("ê¸°ë³´ ë°ì´í„°ë¥¼ í•™ìŠµì— í™œìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
        use_gibo = gibo_choice != 'n'
    
    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì—¬ë¶€
    load_model = None
    existing_models = glob.glob("models/*.json")
    if existing_models:
        print(f"\nê¸°ì¡´ ëª¨ë¸ {len(existing_models)}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
        for i, model in enumerate(existing_models[:5], 1):
            print(f"  {i}. {os.path.basename(model)}")
        
        load_choice = input("\nê¸°ì¡´ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ìˆ«ì ì…ë ¥ ë˜ëŠ” n): ").strip().lower()
        if load_choice != 'n' and load_choice.isdigit():
            idx = int(load_choice) - 1
            if 0 <= idx < len(existing_models):
                load_model = existing_models[idx]
    
    return selected_time, use_gibo, load_model


def train_with_gpu(config: TrainingConfig, load_model: Optional[str] = None, gibo_dir: str = "gibo"):
    """GPU ê°€ì† í•™ìŠµ ì‹¤í–‰"""
    try:
        import torch
        from janggi.nnue_torch import NNUETorch, FeatureExtractor, GPUTrainer, get_device
    except ImportError as e:
        print(f"âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")
        print("ì„¤ì¹˜: pip install torch")
        return None
    
    device = get_device()
    print(f"\nğŸš€ GPU í•™ìŠµ ì‹œì‘ (Device: {device})")
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ
    if load_model:
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {load_model}")
        nnue = NNUETorch.from_file(load_model, device=device)
    else:
        print("ğŸ†• ìƒˆ ëª¨ë¸ ì´ˆê¸°í™”")
        nnue = NNUETorch(device=device)
    
    # ê¸°ë³´ ê¸°ë°˜ í•™ìŠµ
    if config.use_gibo:
        from scripts.train_nnue_gibo import GibParser, GiboDataGenerator, train_with_gradient_clipping
        
        print("\nğŸ“š ê¸°ë³´ íŒŒì¼ íŒŒì‹± ì¤‘...")
        parser = GibParser()
        games = parser.parse_directory(gibo_dir)
        
        if games:
            print(f"âœ… {len(games)}ê°œ ê²Œì„ ë¡œë“œ ì™„ë£Œ")
            
            generator = GiboDataGenerator()
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© (CPU ì½”ì–´ê°€ 4ê°œ ì´ìƒì´ê³  ê²Œì„ì´ ë§ìœ¼ë©´ ìë™ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬)
            import multiprocessing as mp
            cpu_count = mp.cpu_count()
            use_parallel = config.use_parallel and len(games) > 100
            
            if use_parallel:
                print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ ì‚¬ìš© ({config.num_workers}ê°œ ì›Œì»¤)")
                features, targets = generator.generate_from_games_parallel(
                    games,
                    positions_per_game=50,
                    num_workers=config.num_workers,
                    progress_callback=lambda d, t: print(f"\rì²˜ë¦¬ ì¤‘: {d}/{t}", end="", flush=True)
                )
            else:
                print("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ ì‚¬ìš©")
                features, targets = generator.generate_from_games(
                    games,
                    positions_per_game=50,
                    progress_callback=lambda d, t: print(f"\rì²˜ë¦¬ ì¤‘: {d}/{t}", end="", flush=True)
                )
            print()
            
            print(f"\nğŸ“ ê¸°ë³´ ê¸°ë°˜ í•™ìŠµ ì‹œì‘ ({len(features)}ê°œ í¬ì§€ì…˜)...")
            train_with_gradient_clipping(
                nnue, features, targets,
                epochs=config.epochs // 2,
                batch_size=config.batch_size,
                learning_rate=config.learning_rate
            )
    
    # Self-play ë°ì´í„° ìƒì„± ë° í•™ìŠµ
    from scripts.train_nnue_gpu import DataGenerator
    
    generator = DataGenerator()
    
    def progress(done, total, speed, eta):
        print(f"\rğŸ“Š í¬ì§€ì…˜ ìƒì„±: {done:,}/{total:,} ({speed:.1f}/s, ETA: {eta:.0f}s)", end="", flush=True)
    
    print(f"\nğŸ² Self-play í¬ì§€ì…˜ ìƒì„± ì¤‘ ({config.positions:,}ê°œ)...")
    
    if config.use_parallel:
        features, targets = generator.generate_positions_parallel(
            num_positions=config.positions,
            num_workers=config.num_workers,
            progress_callback=progress
        )
    else:
        features, targets = generator.generate_positions_fast(
            num_positions=config.positions,
            progress_callback=progress
        )
    
    print()  # ì¤„ë°”ê¿ˆ
    
    print(f"\nğŸ“ í•™ìŠµ ì‹œì‘ ({len(features):,}ê°œ í¬ì§€ì…˜, {config.epochs}íšŒ ì—í¬í¬)...")
    trainer = GPUTrainer(nnue)
    
    history = trainer.train(
        features, targets,
        epochs=config.epochs,
        batch_size=config.batch_size,
        learning_rate=config.learning_rate,
        early_stopping_patience=10
    )
    
    # ëª¨ë¸ ì €ì¥
    output_path = "models/nnue_smart_model.json"
    os.makedirs("models", exist_ok=True)
    nnue.save(output_path)
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {output_path}")
    
    return nnue, history


def train_with_cpu(config: TrainingConfig, load_model: Optional[str] = None, gibo_dir: str = "gibo"):
    """CPU í•™ìŠµ ì‹¤í–‰"""
    from janggi.nnue import NNUE
    from scripts.train_nnue import TrainingDataGenerator, NNUETrainer, IterativeTrainer
    
    print("\nğŸ–¥ï¸  CPU í•™ìŠµ ì‹œì‘")
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ
    if load_model:
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {load_model}")
        nnue = NNUE.from_file(load_model)
    else:
        print("ğŸ†• ìƒˆ ëª¨ë¸ ì´ˆê¸°í™”")
        nnue = NNUE()
    
    # ë°˜ë³µ í•™ìŠµ ì‚¬ìš©
    if config.iterations > 1:
        print(f"\nğŸ”„ ë°˜ë³µ í•™ìŠµ ëª¨ë“œ ({config.iterations}íšŒ)")
        trainer = IterativeTrainer(nnue)
        trainer.run_iterations(
            num_iterations=config.iterations,
            games_per_iteration=config.positions // (50 * config.iterations),
            search_depth=config.search_depth,
            epochs_per_iteration=config.epochs // config.iterations,
            output_dir="models",
            base_name="nnue_smart_iter"
        )
    else:
        # ë‹¨ì¼ í•™ìŠµ
        print(f"\nğŸ² í¬ì§€ì…˜ ìƒì„± ì¤‘ ({config.positions}ê°œ)...")
        generator = TrainingDataGenerator(search_depth=config.search_depth)
        
        boards, targets = generator.generate_diverse_positions(
            num_positions=config.positions,
            search_depth=config.search_depth,
            progress_callback=lambda c, t: print(f"\rğŸ“Š í¬ì§€ì…˜: {c}/{t}", end="", flush=True)
        )
        print()
        
        print(f"\nğŸ“ í•™ìŠµ ì‹œì‘ ({len(boards)}ê°œ í¬ì§€ì…˜)...")
        trainer = NNUETrainer(nnue)
        history = trainer.train(
            boards, targets,
            epochs=config.epochs,
            learning_rate=config.learning_rate,
            batch_size=config.batch_size
        )
    
    # ëª¨ë¸ ì €ì¥
    output_path = "models/nnue_smart_model.json"
    os.makedirs("models", exist_ok=True)
    nnue.save(output_path)
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {output_path}")
    
    return nnue


def evaluate_model(model, num_games: int = 5):
    """ëª¨ë¸ í‰ê°€"""
    print(f"\nğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘ ({num_games}ê²Œì„)...")
    
    try:
        from scripts.train_nnue_gpu import evaluate_model as gpu_eval
        win_rate = gpu_eval(model, num_games=num_games)
    except:
        # CPU ëª¨ë¸ í‰ê°€
        from janggi.board import Board, Side
        from janggi.engine import Engine
        
        wins = 0
        engine = Engine(depth=2, use_nnue=True)
        engine.nnue = model
        
        for _ in range(num_games):
            board = Board()
            for _ in range(100):
                if board.is_checkmate() or board.is_stalemate():
                    break
                move = engine.search(board)
                if move is None:
                    break
                board.make_move(move)
            
            if board.is_checkmate() and board.side_to_move == Side.HAN:
                wins += 1
        
        win_rate = wins / num_games
    
    print(f"âœ… SimpleEvaluator ëŒ€ë¹„ ìŠ¹ë¥ : {win_rate:.1%}")
    return win_rate


def main():
    parser = argparse.ArgumentParser(
        description='Smart NNUE Training - ìë™ í™˜ê²½ ê°ì§€ ë° ìµœì í™” í•™ìŠµ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
í•™ìŠµ ì‹œê°„ ì˜µì…˜:
  quick      âš¡ ë¹ ë¥¸ í•™ìŠµ (~5ë¶„)    - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
  standard   ğŸ“˜ í‘œì¤€ í•™ìŠµ (~15ë¶„)   - ì¼ë°˜ì ì¸ ì‚¬ìš©
  deep       ğŸ“— ê¹Šì€ í•™ìŠµ (~30ë¶„)   - ê¶Œì¥
  intensive  ğŸ“• ì§‘ì¤‘ í•™ìŠµ (~1ì‹œê°„)  - ë†’ì€ ì„±ëŠ¥
  full       ğŸ† ì™„ì „ í•™ìŠµ (~2ì‹œê°„+) - ìµœê³  ì„±ëŠ¥

ì˜ˆì‹œ:
  python smart_train.py                      # ëŒ€í™”í˜• ëª¨ë“œ
  python smart_train.py --time standard      # í‘œì¤€ í•™ìŠµ
  python smart_train.py --time deep --no-gibo  # ê¸°ë³´ ì—†ì´ ê¹Šì€ í•™ìŠµ
        """
    )
    
    parser.add_argument('--time', type=str, 
                        choices=['quick', 'standard', 'deep', 'intensive', 'full'],
                        default=None,
                        help='í•™ìŠµ ì‹œê°„ ì„ íƒ')
    parser.add_argument('--load', type=str, default=None,
                        help='ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ë¡œë“œ')
    parser.add_argument('--no-gibo', action='store_true',
                        help='ê¸°ë³´ íŒŒì¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ')
    parser.add_argument('--gibo-dir', type=str, default='gibo',
                        help='ê¸°ë³´ íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='models/nnue_smart_model.json',
                        help='ì¶œë ¥ ëª¨ë¸ íŒŒì¼')
    parser.add_argument('--skip-eval', action='store_true',
                        help='í•™ìŠµ í›„ í‰ê°€ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--info-only', action='store_true',
                        help='ì‹œìŠ¤í…œ ì •ë³´ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    print("\nğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ë¶„ì„ ì¤‘...")
    info = get_system_info(args.gibo_dir)
    print_system_info(info)
    
    if args.info_only:
        return
    
    # í•™ìŠµ ì‹œê°„ ì„ íƒ
    if args.time:
        training_time = TrainingTime(args.time)
        use_gibo = not args.no_gibo and info.has_gibo_files
        load_model = args.load
    else:
        # ëŒ€í™”í˜• ë©”ë‰´
        result = interactive_menu(info)
        if result[0] is None:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        training_time, use_gibo, load_model = result
    
    # í•™ìŠµ ì„¤ì • ê³„ì‚°
    config = get_training_config(info, training_time, use_gibo)
    print_training_config(config)
    
    # í™•ì¸
    if not args.time:  # ëŒ€í™”í˜• ëª¨ë“œì—ì„œë§Œ í™•ì¸
        confirm = input("\nì´ ì„¤ì •ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
        if confirm == 'n':
            print("ğŸ‘‹ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    # í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 60)
    print("ğŸ“ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    start_time = time.time()
    
    if config.method in ["gpu", "gpu_gibo"]:
        result = train_with_gpu(config, load_model, args.gibo_dir)
        if result:
            model, history = result
    else:
        model = train_with_cpu(config, load_model, args.gibo_dir)
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸  ì´ í•™ìŠµ ì‹œê°„: {elapsed/60:.1f}ë¶„")
    
    # ëª¨ë¸ í‰ê°€
    if not args.skip_eval and model:
        try:
            evaluate_model(model)
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nëª¨ë¸ ì €ì¥ ìœ„ì¹˜: models/nnue_smart_model.json")
    print("\nì‚¬ìš©ë²•:")
    print("  from janggi.nnue_torch import NNUETorch")
    print("  nnue = NNUETorch.from_file('models/nnue_smart_model.json')")


if __name__ == "__main__":
    main()

