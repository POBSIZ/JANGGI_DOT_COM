# 모델 평가 병목 개선 방안의 Side Effect 분석

## 개요

모델 평가 단계에서 발생하는 병목 지점에 대한 개선 방안과 각 방안을 적용했을 때 예상되는 부작용(side effect)을 분석합니다.

## 현재 병목 지점

1. **각 워커에서 모델 로드** (가장 큰 병목)
   - 위치: `scripts/train_nnue_gpu.py:774-779`
   - 각 워커가 시작될 때마다 모델 파일을 디스크에서 읽고 CPU 메모리에 로드
   - 여러 워커가 동시에 같은 파일을 읽으려 하면 I/O 경합 발생
   - 워커 수만큼 모델이 메모리에 중복 로드

2. **임시 모델 파일 저장**
   - 위치: `scripts/train_nnue_gpu.py:872-878`
   - 모델을 JSON으로 직렬화하고 디스크에 저장하는 오버헤드

3. **작은 게임 수에 대한 오버헤드**
   - 5게임 평가에 3개 워커 사용 시, 워커 초기화 + 모델 로드 시간이 실제 게임 시뮬레이션 시간보다 클 수 있음

4. **멀티프로세싱 풀 초기화**
   - 워커 프로세스 생성 및 초기화 시간

---

## 개선 방안별 Side Effect 분석

### 방안 1: 작은 게임 수에서 워커 수를 1개로 제한

#### 구현 방법
```python
if num_games <= 5:  # 현재는 <= 3
    num_workers = 1
elif num_games <= 10:
    num_workers = min(2, num_games)  # 현재는 min(3, num_games)
else:
    num_workers = max(1, min(mp.cpu_count() - 1, num_games))
```

#### 예상 효과
- ✅ 워커 초기화 오버헤드 감소
- ✅ 모델 로드 횟수 감소 (1회만)
- ✅ I/O 경합 제거
- ✅ 메모리 사용량 감소

#### Side Effects

##### 1. **병렬화 이점 상실 (중요)**
- **문제**: 5-10게임 평가 시 병렬화 이점을 거의 활용하지 못함
- **영향**: 
  - 게임 시뮬레이션이 순차적으로 실행되어 전체 평가 시간이 증가할 수 있음
  - 특히 `search_depth=3`일 때 각 게임당 수백~수천 번의 포지션 평가가 필요하므로 순차 실행 시 시간이 길어짐
- **완화 방안**:
  - 게임 수가 적을 때는 `search_depth`를 낮춰서 평가 시간 단축
  - 또는 게임 수가 적을 때는 평가를 건너뛰고 더 큰 배치에서만 평가 수행

##### 2. **CPU 코어 활용률 저하**
- **문제**: 멀티코어 시스템에서 CPU 코어가 유휴 상태로 남음
- **영향**: 
  - 시스템 리소스 활용률이 낮아짐
  - 다른 작업과 병행 실행 시에는 오히려 유리할 수 있음
- **완화 방안**: 
  - 동적 워커 수 조정: 시스템 부하에 따라 워커 수 결정
  - 사용자 옵션으로 워커 수를 직접 지정 가능하게 함

##### 3. **평가 시간 예측 어려움**
- **문제**: 게임 수에 따라 워커 수가 달라지므로 평가 시간 예측이 어려움
- **영향**: 
  - 진행 상황 표시가 일관되지 않을 수 있음
  - 사용자가 예상 시간을 파악하기 어려움
- **완화 방안**: 
  - 워커 수 변경 시 명확한 로그 메시지 출력
  - 예상 시간 계산 시 워커 수를 고려

##### 4. **임계값 설정의 어려움**
- **문제**: "작은 게임 수"의 기준이 시스템마다 다를 수 있음
- **영향**: 
  - 빠른 시스템에서는 5게임도 병렬화가 유리할 수 있음
  - 느린 시스템에서는 10게임도 순차 실행이 더 빠를 수 있음
- **완화 방안**: 
  - 벤치마크를 통해 시스템별 최적 임계값 자동 결정
  - 환경 변수나 설정 파일로 임계값 조정 가능

---

### 방안 2: 모델을 공유 메모리에 로드하여 워커 간 공유

#### 구현 방법
```python
import multiprocessing as mp
from multiprocessing import shared_memory

# 메인 프로세스에서 모델을 공유 메모리에 저장
model_state = nnue.model.state_dict()
# ... 공유 메모리 설정 ...

# 워커에서 공유 메모리에서 모델 로드
def _evaluate_single_game_worker(args):
    # 공유 메모리에서 모델 읽기
    ...
```

#### 예상 효과
- ✅ 모델 파일 I/O 제거
- ✅ 메모리 중복 로드 방지
- ✅ 워커 초기화 시간 단축

#### Side Effects

##### 1. **구현 복잡도 증가 (심각)**
- **문제**: PyTorch 모델을 공유 메모리에 저장하고 로드하는 것이 복잡함
- **영향**: 
  - PyTorch 텐서는 직접 공유 메모리에 저장할 수 없음
  - NumPy 배열로 변환 후 공유 메모리에 저장해야 함
  - 모델 구조 정보도 별도로 전달해야 함
  - 코드 유지보수성 저하
- **완화 방안**: 
  - `multiprocessing.Manager`를 사용하여 모델 상태를 공유 (더 간단하지만 성능 저하)
  - 또는 모델을 pickle하여 공유 메모리에 저장 (Python 3.8+)

##### 2. **메모리 복사 오버헤드**
- **문제**: 공유 메모리에서 모델을 읽을 때도 메모리 복사가 발생
- **영향**: 
  - 모델 크기가 클수록 복사 시간이 길어짐
  - 각 워커가 모델을 자신의 메모리 공간에 복사해야 함
  - 실제로는 디스크 I/O보다 빠르지만 완전히 제로는 아님
- **완화 방안**: 
  - 작은 모델에서는 효과가 제한적일 수 있음
  - 큰 모델에서만 공유 메모리 사용

##### 3. **플랫폼 호환성 문제**
- **문제**: Windows에서 `multiprocessing.shared_memory`가 제대로 작동하지 않을 수 있음
- **영향**: 
  - Windows에서는 `spawn` 방식을 사용하므로 각 프로세스가 독립적인 메모리 공간을 가짐
  - 공유 메모리가 실제로 공유되지 않을 수 있음
  - 크로스 플랫폼 호환성 저하
- **완화 방안**: 
  - 플랫폼별로 다른 방식 사용 (Windows: 파일 기반, Linux: 공유 메모리)
  - 또는 Windows에서는 방안 1만 사용

##### 4. **모델 업데이트 동기화 문제**
- **문제**: 학습 중 모델이 업데이트되면 공유 메모리의 모델도 업데이트해야 함
- **영향**: 
  - 현재 평가 중인 워커들이 오래된 모델을 사용할 수 있음
  - 동기화 메커니즘 필요 (락, 버전 관리 등)
  - 복잡도 증가
- **완화 방안**: 
  - 평가 중에는 모델을 업데이트하지 않음 (현재 구조에서는 문제 없음)
  - 또는 평가마다 새로 공유 메모리에 모델 저장

##### 5. **디버깅 어려움**
- **문제**: 공유 메모리를 사용하면 디버깅이 어려워짐
- **영향**: 
  - 메모리 상태를 직접 확인하기 어려움
  - 워커 간 데이터 공유 문제를 추적하기 어려움
  - 예외 발생 시 스택 트레이스가 복잡함
- **완화 방안**: 
  - 상세한 로깅 추가
  - 공유 메모리 사용 여부를 옵션으로 제공

---

### 방안 3: 모델 로드 결과를 캐싱하여 재사용

#### 구현 방법
```python
# 전역 캐시 (모듈 레벨)
_model_cache = {}

def _evaluate_single_game_worker(args):
    global _model_cache
    model_path = args[1]
    
    if model_path not in _model_cache:
        _model_cache[model_path] = NNUETorch.from_file(model_path, device=torch.device('cpu'))
    
    nnue_model = _model_cache[model_path]
    ...
```

#### 예상 효과
- ✅ 같은 모델 파일을 여러 워커가 사용할 때 중복 로드 방지
- ✅ 구현이 간단함

#### Side Effects

##### 1. **멀티프로세싱에서 캐시가 작동하지 않음 (치명적)**
- **문제**: Python의 `multiprocessing`은 각 워커가 독립적인 프로세스이므로 전역 변수를 공유하지 않음
- **영향**: 
  - 각 워커가 독립적인 `_model_cache`를 가지므로 캐시 효과가 없음
  - 실제로는 모델을 여전히 각 워커에서 로드해야 함
  - 코드만 복잡해지고 성능 향상은 없음
- **완화 방안**: 
  - **이 방안은 멀티프로세싱 환경에서는 효과가 없으므로 적용하지 않는 것이 좋음**
  - 단일 프로세스 환경이나 threading을 사용하는 경우에만 효과적

##### 2. **메모리 누수 위험**
- **문제**: 캐시에 모델이 계속 쌓이면 메모리 사용량이 증가
- **영향**: 
  - 여러 모델을 평가할 때 각 모델이 캐시에 남아있음
  - 장시간 실행 시 메모리 부족 발생 가능
- **완화 방안**: 
  - LRU 캐시 사용 (최근 사용한 모델만 유지)
  - 캐시 크기 제한
  - 명시적 캐시 클리어 메커니즘

##### 3. **모델 파일 변경 감지 문제**
- **문제**: 모델 파일이 업데이트되어도 캐시에는 이전 모델이 남아있음
- **영향**: 
  - 오래된 모델을 사용하여 평가할 수 있음
  - 잘못된 평가 결과
- **완화 방안**: 
  - 파일 수정 시간을 확인하여 캐시 무효화
  - 또는 파일 경로에 타임스탬프 포함

##### 4. **동시성 문제 (threading 환경에서)**
- **문제**: 여러 스레드가 동시에 캐시에 접근하면 경합 조건 발생 가능
- **영향**: 
  - 모델이 중복 로드될 수 있음
  - 예기치 않은 동작
- **완화 방안**: 
  - 락(lock) 사용
  - `threading.local()` 사용

---

## 종합 권장 사항

### 즉시 적용 가능한 방안
1. **방안 1 개선**: 작은 게임 수에서 워커 수를 더 적극적으로 제한
   - `num_games <= 5`일 때 `num_workers = 1`
   - `num_games <= 10`일 때 `num_workers = min(2, num_games)`
   - **Side Effect**: 병렬화 이점 상실 (하지만 작은 게임 수에서는 오버헤드가 더 큼)

### 검토가 필요한 방안
2. **방안 2 (공유 메모리)**: 구현 복잡도가 높고 플랫폼 호환성 문제가 있음
   - **권장**: 현재는 적용하지 않고, 향후 모델이 매우 클 때만 고려
   - **대안**: 모델을 더 작게 만들거나 양자화(quantization) 고려

### 적용하지 않는 방안
3. **방안 3 (캐싱)**: 멀티프로세싱 환경에서는 효과가 없음
   - **권장**: 적용하지 않음

---

## 추가 개선 방안

### 방안 4: 모델 파일 저장 최적화
- **방법**: JSON 대신 PyTorch의 `.pt` 형식 사용 (더 빠른 로드)
- **Side Effect**: 
  - 기존 JSON 모델과 호환성 문제 (마이그레이션 필요)
  - 파일 크기는 비슷하지만 로드 속도는 향상

### 방안 5: 워커 풀 재사용
- **방법**: 워커 풀을 한 번 생성하고 여러 평가에 재사용
- **Side Effect**: 
  - 워커가 모델을 한 번만 로드하고 재사용
  - 하지만 모델이 업데이트되면 워커를 재시작해야 함
  - 메모리 사용량 증가 (워커가 계속 살아있음)

### 방안 6: 게임 시뮬레이션과 평가 분리
- **방법**: 워커는 게임만 생성하고, 평가는 메인 프로세스에서 GPU 배치로 수행
- **Side Effect**: 
  - 워커에서 모델 로드 불필요
  - 하지만 현재 구조와 많이 다르므로 대규모 리팩토링 필요
  - 이미 `generate_selfplay_data_parallel`에서 사용 중인 패턴

---

## 결론

**가장 실용적인 개선 방안은 방안 1의 개선**입니다:
- 구현이 간단함
- 즉시 적용 가능
- Side Effect가 명확하고 관리 가능
- 작은 게임 수에서는 실제로 더 빠를 수 있음

**방안 2와 3은 현재 상황에서는 권장하지 않습니다:**
- 방안 2: 구현 복잡도가 높고 플랫폼 호환성 문제
- 방안 3: 멀티프로세싱 환경에서 효과 없음

**장기적으로는 방안 6 (게임 시뮬레이션과 평가 분리)**을 고려할 수 있지만, 이는 대규모 리팩토링이 필요합니다.

